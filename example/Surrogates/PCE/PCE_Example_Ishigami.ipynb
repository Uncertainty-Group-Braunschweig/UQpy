{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Chaos Expansion example: Ishigami function (3 random inputs, scalar output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors: Dimitrios Loukrezis \\ \n",
    "Date: May 6 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we approximate the well-known Ishigami function with a total-degree Polynomial Chaos Expansion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The selected optimizer method does not support bounds and thus will be ignored.\n",
      "The selected optimizer method does not support bounds and thus will be ignored.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import numpy as np\n",
    "from UQpy.distributions import Uniform, JointIndependent\n",
    "from UQpy.surrogates import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define the Ishigami function, which reads:\n",
    "$$f(x_1, x_2, x_3) = \\sin(x_1) + a \\sin^2(x_2) + b x_3^4 \\sin(x_1).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to be approximated\n",
    "def ishigami(xx):\n",
    "    \"\"\"Ishigami function\"\"\"\n",
    "    a = 7\n",
    "    b = 0.1\n",
    "    term1 = np.sin(xx[0])\n",
    "    term2 = a * np.sin(xx[1])**2\n",
    "    term3 = b * xx[2]**4 * np.sin(xx[0])\n",
    "    return term1 + term2 + term3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Ishigami function has three random inputs, which are uniformly distributed in $[-\\pi, \\pi]$. Moreover, the input random variables are mutually independent, which simplifies the construction of the joint distribution. Let's define the corresponding distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input distributions\n",
    "dist1 = Uniform(loc=-np.pi, scale=2*np.pi)    \n",
    "dist2 = Uniform(loc=-np.pi, scale=2*np.pi)\n",
    "dist3 = Uniform(loc=-np.pi, scale=2*np.pi)    \n",
    "marg = [dist1, dist2, dist3]\n",
    "joint = JointIndependent(marginals=marg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define our PCE. Only thing we need is the joint distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define PCE metamodel\n",
    "pce_metamodel = PolyChaosExp(joint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must now select a polynomial basis. Here we opt for a total-degree (TD) basis, such that the univariate polynomials have a maximum degree equal to $P$ and all multivariate polynomial have a total-degree (sum of degrees of corresponding univariate polynomials) at most equal to $P$. The size of the basis is then given by \n",
    "$$\\frac{(N+P)!}{N! P!},$$\n",
    "where $N$ is the number of random inputs (here, $N+3$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of PCE basis: 84\n"
     ]
    }
   ],
   "source": [
    "# maximum polynomial degree\n",
    "P = 6  \n",
    "\n",
    "# construct total-degree polynomial basis\n",
    "construct_td_basis(pce_metamodel, P)\n",
    "\n",
    "# check the size of the basis\n",
    "print('Size of PCE basis:', pce_metamodel.n_polys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must now compute the PCE coefficients. For that we first need a training sample of input random variable realizations and the corresponding model outputs. These two data sets form what is also known as an ''experimental design''. It is generally advisable that the experimental design has $2-10$ times more data points than the number of PCE polynomials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of experimental design: 420\n"
     ]
    }
   ],
   "source": [
    "# create training data\n",
    "sample_size = int(pce_metamodel.n_polys*5)\n",
    "print('Size of experimental design:', sample_size)\n",
    "\n",
    "# realizations of random inputs\n",
    "xx_train = joint.rvs(sample_size)\n",
    "# corresponding model outputs\n",
    "yy_train = np.array([ishigami(x) for x in xx_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now fit the PCE coefficients by solving a regression problem. There are multiple ways to do this, e.g. least squares regression, ridge regression, LASSO regression, etc. Here we opt for the _np.linalg.lstsq_ method, which is based on the _dgelsd_ solver of LAPACK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "fit_lstsq(pce_metamodel, xx_train, yy_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By simply post-processing the PCE's terms, we are able to get estimates regarding the mean and standard deviation of the model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCE mean estimate: [3.48873447]\n",
      "PCE variance estimate: [14.01257008]\n"
     ]
    }
   ],
   "source": [
    "mean_est = pce_mean(pce_metamodel)\n",
    "var_est = pce_variance(pce_metamodel)\n",
    "print('PCE mean estimate:', mean_est)\n",
    "print('PCE variance estimate:', var_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to the mean and variance estimates, we can very simply estimate the Sobol sensitivity indices, which quantify the importance of the input random variables in terms of impact on the model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First-order Sobol indices:\n",
      "[[0.31391502]\n",
      " [0.43182407]\n",
      " [0.00050566]]\n",
      "Total-order Sobol indices:\n",
      "[[0.56487066]\n",
      " [0.44163132]\n",
      " [0.25151004]]\n"
     ]
    }
   ],
   "source": [
    "sobol_first = pce_sobol_first(pce_metamodel)\n",
    "sobol_total = pce_sobol_total(pce_metamodel)\n",
    "print('First-order Sobol indices:')\n",
    "print(sobol_first)\n",
    "print('Total-order Sobol indices:')\n",
    "print(sobol_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PCE should become increasingly more accurate as the maximum polynomial degree $P$ increases. We will test that by computing the mean absolute error (MAE) between the PCE's predictions and the true model outputs, given a validation sample of $10^5$ data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial degree: 0\n",
      "Mean absolute error: 3.5092014513593974\n",
      " \n",
      "Polynomial degree: 1\n",
      "Mean absolute error: 2.9190940130421446\n",
      " \n",
      "Polynomial degree: 2\n",
      "Mean absolute error: 2.881426057510191\n",
      " \n",
      "Polynomial degree: 3\n",
      "Mean absolute error: 2.490517639729513\n",
      " \n",
      "Polynomial degree: 4\n",
      "Mean absolute error: 1.6837629839128996\n",
      " \n",
      "Polynomial degree: 5\n",
      "Mean absolute error: 1.4288047926039877\n",
      " \n",
      "Polynomial degree: 6\n",
      "Mean absolute error: 0.47225689340879734\n",
      " \n",
      "Polynomial degree: 7\n",
      "Mean absolute error: 0.3275845356142154\n",
      " \n",
      "Polynomial degree: 8\n",
      "Mean absolute error: 0.068521038112655\n",
      " \n",
      "Polynomial degree: 9\n",
      "Mean absolute error: 0.04424218398187098\n",
      " \n",
      "Polynomial degree: 10\n",
      "Mean absolute error: 0.0049732041095257515\n",
      " \n",
      "Polynomial degree: 11\n",
      "Mean absolute error: 0.0038931663654605785\n",
      " \n",
      "Polynomial degree: 12\n",
      "Mean absolute error: 0.00025446069874273823\n",
      " \n",
      "Polynomial degree: 13\n",
      "Mean absolute error: 0.0002543042852670587\n",
      " \n",
      "Polynomial degree: 14\n",
      "Mean absolute error: 1.118914790000318e-05\n",
      " \n",
      "Polynomial degree: 15\n",
      "Mean absolute error: 1.0716808549687988e-05\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# validation data sets\n",
    "np.random.seed(999) # fix random seed for reproducibility\n",
    "n_samples_val = 100000\n",
    "xx_val = joint.rvs(n_samples_val)\n",
    "yy_val = np.array([ishigami(x) for x in xx_val])\n",
    "\n",
    "mae = [] # to hold MAE for increasing polynomial degree\n",
    "for degree in range(16):\n",
    "    # define PCE\n",
    "    pce_metamodel = PolyChaosExp(joint)\n",
    "    \n",
    "    # build PCE basis\n",
    "    construct_td_basis(pce_metamodel, degree)\n",
    "    \n",
    "    # create training data\n",
    "    np.random.seed(1) # fix random seed for reproducibility\n",
    "    sample_size = int(pce_metamodel.n_polys*5)\n",
    "    xx_train = joint.rvs(sample_size)\n",
    "    yy_train = np.array([ishigami(x) for x in xx_train])\n",
    "    \n",
    "    # fit PCE coefficients\n",
    "    fit_lstsq(pce_metamodel, xx_train, yy_train)\n",
    "    \n",
    "    # compute mean absolute validation error\n",
    "    yy_val_pce = pce_metamodel.predict(xx_val).flatten()\n",
    "    errors = np.abs(yy_val.flatten() - yy_val_pce)\n",
    "    mae.append(np.linalg.norm(errors, 1)/n_samples_val)\n",
    "    \n",
    "    print('Polynomial degree:', degree)\n",
    "    print('Mean absolute error:', mae[-1])\n",
    "    print(' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
